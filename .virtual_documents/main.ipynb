# Cell 2 — Imports & paths
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import pickle

from mlxtend.frequent_patterns import apriori, association_rules

# Uploaded file path
UPLOADED_USERS_CSV = './data.xlsx'
OUTPUT_DIR = '/mnt/data/project_outputs'
os.makedirs(OUTPUT_DIR, exist_ok=True)



# Cell 3 — Load dataset
df = pd.read_excel(UPLOADED_USERS_CSV)
print("Shape:", df.shape)
df.head()


# Cell 4 — Clean prices and discount
def parse_price(x):
    try:
        if pd.isna(x): return np.nan
        s = str(x).replace('₹','').replace(',','').replace('â','').strip()
        return float(s)
    except:
        return np.nan

for col in ['discounted_price','actual_price']:
    if col in df.columns:
        df[col] = df[col].apply(parse_price)

def parse_pct(x):
    try:
        if pd.isna(x): return 0.0
        return float(str(x).replace('%','').strip())
    except:
        return 0.0

if 'discount_percentage' in df.columns:
    df['discount_percentage'] = df['discount_percentage'].apply(parse_pct)

df['rating'] = pd.to_numeric(df['rating'], errors='coerce').fillna(3.0)
df['price_diff'] = df['actual_price'] - df['discounted_price']
df['category_top'] = df['category'].apply(lambda x: str(x).split('|')[0] if pd.notna(x) else 'Unknown')
df.head()


# Cell 5 — User-level aggregation
user_agg = df.groupby('user_id').agg(
    user_name=('user_name','first'),
    avg_rating_given=('rating','mean'),
    review_count=('review_id','count'),
    avg_discount_seen=('discount_percentage','mean'),
    avg_price_seen=('discounted_price','mean')
).reset_index()

user_agg['avg_rating_given'] = user_agg['avg_rating_given'].fillna(user_agg['avg_rating_given'].mean())
user_agg['avg_discount_seen'] = user_agg['avg_discount_seen'].fillna(0.0)
user_agg['avg_price_seen'] = user_agg['avg_price_seen'].fillna(user_agg['avg_price_seen'].median())
user_agg.head()



# Cell 6 — Segmentation with KMeans + PCA
features = ['avg_rating_given','review_count','avg_discount_seen','avg_price_seen']
X = user_agg[features].copy()
X['review_count_log'] = np.log1p(X['review_count'])
X_model = X[['avg_rating_given','review_count_log','avg_discount_seen','avg_price_seen']].fillna(0)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_model)

# PCA for viz
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)
user_agg['pca1'] = X_pca[:,0]
user_agg['pca2'] = X_pca[:,1]

# silhouette score to find best K
sil_scores = {}
for k in range(2,8):
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = km.fit_predict(X_scaled)
    sil_scores[k] = silhouette_score(X_scaled, labels)
print("Silhouette scores:", sil_scores)

best_k = max(sil_scores, key=sil_scores.get)
print("Best cluster count:", best_k)

kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
user_agg['cluster'] = kmeans.fit_predict(X_scaled)

# Plot clusters
plt.figure(figsize=(8,6))
plt.scatter(user_agg['pca1'], user_agg['pca2'], c=user_agg['cluster'], cmap='tab10', alpha=0.6)
plt.xlabel('PCA1'); plt.ylabel('PCA2'); plt.title('User clusters')
plt.grid(True); plt.show()



# Cell 7 — Recommender System using TruncatedSVD (matrix factorization)
ratings_df = df[['user_id','product_id','rating']].copy().dropna()
ratings_matrix = ratings_df.pivot_table(index='user_id', columns='product_id', values='rating').fillna(0)

svd = TruncatedSVD(n_components=20, random_state=42)
latent_matrix = svd.fit_transform(ratings_matrix)
product_matrix = svd.components_.T

print("Latent matrix shape:", latent_matrix.shape, "Product matrix shape:", product_matrix.shape)

# Function: recommend top-N for a user
def recommend_products(user_id, n=5):
    if user_id not in ratings_matrix.index:
        return []
    user_idx = ratings_matrix.index.get_loc(user_id)
    scores = np.dot(latent_matrix[user_idx], product_matrix.T)
    rated_products = ratings_matrix.loc[user_id]
    rated_indices = rated_products[rated_products>0].index
    scores = pd.Series(scores, index=ratings_matrix.columns)
    scores = scores.drop(rated_indices, errors='ignore')
    return scores.sort_values(ascending=False).head(n)

# Example
sample_user = ratings_matrix.index[0]
print("Top recommendations for user:", sample_user)
print(recommend_products(sample_user, n=5))



# Cell 8 — Association Rules (Market Basket Analysis)
from mlxtend.frequent_patterns import apriori, association_rules

basket = ratings_df.copy()
basket['count'] = 1

# Pivot: user-product matrix
user_product = basket.groupby(['user_id','product_id'])['count'].sum().unstack(fill_value=0)

# Convert to boolean
user_product = user_product > 0

# Filter for popular products (appeared at least 5 times)
product_popularity = user_product.sum(axis=0).sort_values(ascending=False)
top_products = product_popularity[product_popularity >= 5].index.tolist()
user_product_top = user_product[top_products]

# Generate frequent itemsets
frequent_itemsets = apriori(user_product_top, min_support=0.002, use_colnames=True)

# Association rules (new version requires `num_itemsets`)
rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0, num_itemsets=len(frequent_itemsets))

# Show top rules
rules.sort_values('lift', ascending=False).head(10)



print("Number of frequent itemsets:", len(frequent_itemsets))
print(frequent_itemsets.head())



# Cell 9 — Save outputs for portfolio
user_agg.to_csv(os.path.join(OUTPUT_DIR,'user_segments.csv'), index=False)
ratings_matrix.to_csv(os.path.join(OUTPUT_DIR,'ratings_matrix.csv'))
frequent_itemsets.to_csv(os.path.join(OUTPUT_DIR,'frequent_itemsets.csv'), index=False)
rules.to_csv(os.path.join(OUTPUT_DIR,'association_rules.csv'), index=False)

with open(os.path.join(OUTPUT_DIR,'kmeans_model.pkl'),'wb') as f:
    pickle.dump(kmeans,f)
with open(os.path.join(OUTPUT_DIR,'svd_model.pkl'),'wb') as f:
    pickle.dump(svd,f)

print("Saved artifacts in:", OUTPUT_DIR)




